{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "df = pd.read_json('News_Category_Dataset.json',lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data       category\n",
      "0  There Were 2 Mass Shootings In Texas Last Week...          CRIME\n",
      "1  Will Smith Joins Diplo And Nicky Jam For The 2...  ENTERTAINMENT\n",
      "2  Hugh Grant Marries For The First Time At Age 5...  ENTERTAINMENT\n",
      "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  ENTERTAINMENT\n",
      "4  Julianna Margulies Uses Donald Trump Poop Bags...  ENTERTAINMENT\n"
     ]
    }
   ],
   "source": [
    "df['data'] = df['headline']+str('. ')+df['short_description']\n",
    "#print(df['data'].head())\n",
    "df1=df[['data','category']]\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  category\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...         6\n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...         8\n",
       "2  Hugh Grant Marries For The First Time At Age 5...         8\n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...         8\n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...         8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "df1['category']  =le.fit_transform(df1['category'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ARTS', u'ARTS & CULTURE', u'BLACK VOICES', u'BUSINESS', u'COLLEGE', u'COMEDY', u'CRIME', u'EDUCATION', u'ENTERTAINMENT', u'FIFTY', u'GOOD NEWS', u'GREEN', u'HEALTHY LIVING', u'IMPACT', u'LATINO VOICES', u'MEDIA', u'PARENTS', u'POLITICS', u'QUEER VOICES', u'RELIGION', u'SCIENCE', u'SPORTS', u'STYLE', u'TASTE', u'TECH', u'THE WORLDPOST', u'TRAVEL', u'WEIRD NEWS', u'WOMEN', u'WORLD NEWS', u'WORLDPOST']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(le.classes_))\n",
    "le.transform(list(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124989,)\n"
     ]
    }
   ],
   "source": [
    "training_data = df1\n",
    "training_data.to_csv(\"train_data.csv\", sep=',', encoding='utf-8')\n",
    "print(training_data.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.data)\n",
    "\n",
    "#SAVE WORD VECTOR\n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#TRANSFORM WORD VECTOR TO TF IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#SAVE TF-IDF\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, training_data.flag)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.category, test_size=0.25, random_state=42)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "#SAVE MODEL\n",
    "pickle.dump(clf, open(\"nb_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#category_list = [\"sport\", \"world\", \"us\", \"business\", \"health\", \"entertainment\", \"sci_tech\"]\n",
    "\n",
    "docs_new = \"Messi killed 3 persons\"\n",
    "docs_new = [docs_new]\n",
    "\n",
    "#LOAD MODEL\n",
    "loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "loaded_model = pickle.load(open(\"nb_model.pkl\",\"rb\"))\n",
    "\n",
    "X_new_counts = loaded_vec.transform(docs_new)\n",
    "X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "predicted = loaded_model.predict(X_new_tfidf)\n",
    "print(predicted)\n",
    "# print(category_list[predicted[0]])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11084"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = loaded_model.predict(X_test)\n",
    "result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_bayes.to_csv('res_bayes.csv', sep = ',')\n",
    "y_test = y_test.astype(int)\n",
    "count = 0\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    #print(predicted_item, ' - ',result)\n",
    "    if(predicted_item==result):\n",
    "        count = count + 1\n",
    "print(len(y_test))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0  111    0    0    0    0    0\n",
      "     0    0    0  273    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  105    0    0    0    0    0\n",
      "     0    0    0  239    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  236    0    0    0    2    0\n",
      "     0    0    0  731    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    4    0    0    0    0   22    0    0    0    6    0\n",
      "     0    0    0 1038    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    4    0    0    0    0    0\n",
      "     0    0    0  284    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    1    0    0    0    0    0  252    0    0    0    2    0\n",
      "     0    0    0  758    0    0    0    0    0    1    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    4    0   21    0    0    0    0    0\n",
      "     0    0    0  670    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    4    0    0    0    0    0\n",
      "     0    0    0  223    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0 2527    0    0    0    0    0\n",
      "     0    0    0 1112    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   21    0    0    0   13    0\n",
      "     0    0    0  310    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   72    0    0    0    2    0\n",
      "     0    0    0  307    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   27    0    0    2    1    0\n",
      "     0    0    0  634    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   46    0    0    0  251    0\n",
      "     0    0    0 1372    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   14    0    0    0    3    0\n",
      "     0    0    0  629    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   61    0    0    0    0    0\n",
      "     0    0    0  218    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   33    0    0    0    0    0\n",
      "     0    1    0  691    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  188    0    0    0    8    0\n",
      "     0    0   30  724    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    1    0    0    0    0   26    0    0    0    0    0\n",
      "     0    0    0 8085    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  218    0    0    0    1    0\n",
      "     0    0    0  971   44    0    0    1    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   28    0    0    0    7    0\n",
      "     0    0    0  578    0   28    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   37    0    0    0    8    0\n",
      "     0    0    0  301    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  152    0    0    0    0    0\n",
      "     0    0    0  807    0    0    0   45    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  244    0    0    0    3    0\n",
      "     0    0    0  339    0    0    0    0    1    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   66    0    0    0   23    0\n",
      "     0    0    0  385    0    0    0    0    0   30    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   16    0    0    0    1    0\n",
      "     0    0    0  300    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    1    0   14    0    0    0    0    0\n",
      "     0    0    0  916    0    0    0    0    0    0    0    9    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   52    0    0    0    3    0\n",
      "     0    0    0  487    0    0    0    0    0    0    0    0    1    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    2    0  120    0    0    0    1    0\n",
      "     0    0    0  532    0    0    0    0    0    0    0    0    0    1\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   96    0    0    0    4    0\n",
      "     0    0    0  755    0    0    0    0    0    0    0    0    0    0\n",
      "    20    0    0]\n",
      " [   0    0    0    0    0    0    0    0   12    0    0    0    0    0\n",
      "     0    0    0  551    0    0    0    0    0    0    0    2    0    0\n",
      "     0    0    0]\n",
      " [   0    0    0    0    0    0    0    0   14    0    0    0    2    0\n",
      "     0    0    0  610    0    0    0    0    0    0    0    8    0    0\n",
      "     0    0    1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.category, test_size=0.25, random_state=42)\n",
    "\n",
    "clf_neural.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16091"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf_neural.predict(X_test)\n",
    "result_softmax = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_softmax.to_csv('res_softmax.csv', sep = ',')\n",
    "y_test = y_test.astype(int)\n",
    "count = 0\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    #print(predicted_item, ' - ',result)\n",
    "    if(predicted_item==result):\n",
    "        count = count + 1\n",
    "print(len(y_test))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "clf_svm = svm.LinearSVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.category, test_size=0.25, random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, training_data.category)\n",
    "pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31248\n",
      "28739\n"
     ]
    }
   ],
   "source": [
    "predicted = clf_svm.predict(X_test)\n",
    "result_svm = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_svm.to_csv('res_svm.csv', sep = ',')\n",
    "y_test = y_test.astype(int)\n",
    "count = 0\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    #print(predicted_item, ' - ',result)\n",
    "    if(predicted_item==result):\n",
    "        count = count + 1\n",
    "print(len(y_test))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.97375328, 0.98442368, 0.9158361 , 0.92721834, 0.94285714,\n",
       "        0.92638732, 0.94049347, 0.97029703, 0.91009755, 0.95439739,\n",
       "        0.95224719, 0.92042042, 0.8733674 , 0.93684211, 0.9832636 ,\n",
       "        0.93465046, 0.89311408, 0.8984339 , 0.9473251 , 0.93610224,\n",
       "        0.95770393, 0.92746615, 0.96509599, 0.94787645, 0.95737705,\n",
       "        0.91666667, 0.96383363, 0.96979332, 0.90464241, 0.97430407,\n",
       "        0.92941176]),\n",
       " array([0.96614583, 0.91860465, 0.85345717, 0.86915888, 0.91666667,\n",
       "        0.80670611, 0.9323741 , 0.86343612, 0.94861226, 0.85174419,\n",
       "        0.88976378, 0.92319277, 0.92150989, 0.82662539, 0.84229391,\n",
       "        0.84827586, 0.91473684, 0.96178501, 0.93198381, 0.91419657,\n",
       "        0.91618497, 0.95517928, 0.94207836, 0.97420635, 0.92113565,\n",
       "        0.91276596, 0.98158379, 0.92987805, 0.824     , 0.80530973,\n",
       "        0.87086614]),\n",
       " array([0.96993464, 0.95037594, 0.88354701, 0.89725036, 0.92957746,\n",
       "        0.86241434, 0.93641618, 0.91375291, 0.92895587, 0.90015361,\n",
       "        0.91994573, 0.92180451, 0.896793  , 0.87828947, 0.90733591,\n",
       "        0.88937093, 0.90379615, 0.92903072, 0.93959184, 0.92501973,\n",
       "        0.93648449, 0.94111874, 0.95344828, 0.96086106, 0.93890675,\n",
       "        0.91471215, 0.97262774, 0.94941634, 0.86244019, 0.88178295,\n",
       "        0.89918699]),\n",
       " array([ 384,  344,  969, 1070,  288, 1014,  695,  227, 3639,  344,  381,\n",
       "         664, 1669,  646,  279,  725,  950, 8112, 1235,  641,  346, 1004,\n",
       "         587,  504,  317,  940,  543,  656,  875,  565,  635], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 17 20 ... 17 19 17]\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
